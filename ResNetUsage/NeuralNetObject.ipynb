{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Try to create a class Of Neural Network that allows me to organize my code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104.00698793, 116.66876762, 122.67891434])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import caffe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import copy #targetpoint\n",
    "\n",
    "# set display defaults\n",
    "plt.rcParams['figure.figsize'] = (10, 10)        # large images\n",
    "plt.rcParams['image.interpolation'] = 'nearest'  # don't interpolate: show square pixels\n",
    "plt.rcParams['image.cmap'] = 'gray'  # use grayscale output rather than a (potentially misleading) color heatmap\n",
    "\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(0)\n",
    "\n",
    "mu = np.load('//home//student401//anaconda3//pkgs//caffe-1.0-py36hdb41b07_3//lib//python3.6//site-packages//caffe//imagenet//ilsvrc_2012_mean.npy')\n",
    "mu = mu.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaffeNet():\n",
    "    def __init__(self, net_def_path, net_weight_path):\n",
    "        self.__def_path = net_def_path\n",
    "        self.__weights_path = net_weight_path\n",
    "        \n",
    "        self.__net = None\n",
    "        self.__transformer = None\n",
    "        \n",
    "        self.__init_net_transformer()\n",
    "    \n",
    "    \n",
    "    def get_inner_net(self):\n",
    "        return self.__net\n",
    "        \n",
    "        \n",
    "    def __init_net_transformer(self):\n",
    "        net = caffe.Net(self.__def_path, self.__weights_path, caffe.TEST)     \n",
    "\n",
    "        #transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "        #transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "        #transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "        #transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "        #transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR\n",
    "        \n",
    "        self.__net = net\n",
    "        #self.__transformer = transformer\n",
    "        \n",
    "        \n",
    "    # FUNCTION DOESN'T USE SELF PARAMETER\n",
    "        \n",
    "    def __show_progress(percent, message = 'Progress'):\n",
    "        clear_output(1)\n",
    "        hash_count = round(50 * percent)\n",
    "        hash_string = ''\n",
    "        \n",
    "        for i in range(hash_count):\n",
    "            hash_string += '#'\n",
    "        \n",
    "        for i in range(50 - hash_count):\n",
    "            hash_string += '-'\n",
    "            \n",
    "        print(message+': '+hash_string)\n",
    "        \n",
    "        \n",
    "    def load_multiple_images(image_path_arr):\n",
    "        \"\"\"\n",
    "        Load images to python list.\n",
    "        Then return this list.\n",
    "        \"\"\"\n",
    "        image_arr = []\n",
    "        i = 0\n",
    "        count = len(image_path_arr)\n",
    "        \n",
    "\n",
    "        for image_path in image_path_arr:\n",
    "            image_arr.append(caffe.io.load_image(image_path, color=False))\n",
    "            i += 1\n",
    "            CaffeNet.__show_progress(i/count, 'Loading images')\n",
    "\n",
    "        return image_arr\n",
    "    \n",
    "    def convert_full_data_to_simple(answer_full_data_arr_l):\n",
    "        \"\"\"\n",
    "        Convert full data to standart (label, confidence) list\n",
    "        \"\"\"\n",
    "        answer_arr_l = []\n",
    "\n",
    "        for layer_data_l in answer_full_data_arr_l:\n",
    "            data_tuple_l = (layer_data_l.argmax(), layer_data_l.max())\n",
    "            answer_arr_l.append(data_tuple_l)\n",
    "\n",
    "        return answer_arr_l\n",
    "    \n",
    "    \n",
    "    # SIMPLE DATA FUNCTIONS\n",
    "    \n",
    "        \n",
    "    def predict(self, image_path):\n",
    "        \"\"\"\n",
    "        Predict single image.\n",
    "        Function returns a tuple - (confidence, label)\n",
    "        \"\"\"\n",
    "        image = caffe.io.load_image(image_path)\n",
    "\n",
    "        transformed_image = self.__transformer.preprocess('data',image)\n",
    "        self.__net.blobs['data'].data[...] = transformed_image\n",
    "        output = self.__net.forward()\n",
    "        output_prob = output['prob'][0]\n",
    "        return (output_prob.argmax(), output_prob.max())\n",
    "     \n",
    "        \n",
    "    def __predict_transfromed_image(self, transformed_image):\n",
    "        \"\"\"\n",
    "        Predict and return (label, confidence)\n",
    "        \"\"\"\n",
    "        self.__net.blobs['data'].data[...] = transformed_image\n",
    "        output = self.__net.forward()\n",
    "        output_prob = output['prob'][0]\n",
    "        return (output_prob.argmax(), output_prob.max())\n",
    "    \n",
    "        \n",
    "    def __transform_image_arr(self, image_arr):\n",
    "        \"\"\"\n",
    "        Transform each image in array and return an array of transformed images\n",
    "        \"\"\"\n",
    "        transformed_image_arr_t = []\n",
    "        i = 0\n",
    "        count = len(image_arr)\n",
    "        \n",
    "        for image_t in image_arr:\n",
    "            transformed_image_arr_t.append(self.__transformer.preprocess('data',image_t))\n",
    "            i += 1\n",
    "            CaffeNet.__show_progress(i/count, 'Transforming')\n",
    "            \n",
    "        return transformed_image_arr_t;\n",
    "    \n",
    "        \n",
    "    def predict_multi(self, image_path_arr, image_arr = 0):\n",
    "        \"\"\"\n",
    "        Predict multiple images.\n",
    "        Function returns a list of tuples - (label, confidence)\n",
    "        \"\"\"\n",
    "        if image_arr == 0:\n",
    "            image_arr = CaffeNet.load_multiple_images(image_path_arr)\n",
    "            \n",
    "        transformed_image_arr_r = image_arr\n",
    "\n",
    "        answers = []\n",
    "        i = 0\n",
    "        count = len(image_path_arr)\n",
    "\n",
    "        for transformed_image_r in transformed_image_arr_r:\n",
    "            answers.append(self.__predict_transfromed_image(transformed_image_r))\n",
    "            i += 1\n",
    "            CaffeNet.__show_progress(i/count, 'Predicting')\n",
    "            \n",
    "        return answers\n",
    "    \n",
    "    \n",
    "    # FULL DATA FUNCTIONS\n",
    "    \n",
    "    \n",
    "    def predict_multi_full_data(self, image_path_arr, image_arr = 0):\n",
    "        \"\"\"\n",
    "        Predict multiple images.\n",
    "        Function returns a list of lists which contain last layer values\n",
    "        \"\"\"\n",
    "        \n",
    "        if image_arr == 0:\n",
    "            input('F')\n",
    "            image_arr = CaffeNet.load_multiple_images(image_path_arr)\n",
    "\n",
    "        transformed_image_arr_p = image_arr\n",
    "        \n",
    "        answer_arr_p = []\n",
    "        \n",
    "        i = 0\n",
    "        count = len(image_arr)\n",
    "        \n",
    "        for transformed_image_p in transformed_image_arr_p:\n",
    "            answer_arr_p.append(copy.copy(self.__predict_transfromed_image_full_data(transformed_image_p))) #targetpoint\n",
    "            i += 1\n",
    "            CaffeNet.__show_progress(i/count, 'Predicting')\n",
    "            \n",
    "        return answer_arr_p\n",
    "    \n",
    "    def __predict_transfromed_image_full_data(self, transformed_image_l):\n",
    "        \"\"\"\n",
    "        Predict and return values of the last layer\n",
    "        \"\"\"\n",
    "        self.__net.blobs['data'].data[...] = transformed_image_l\n",
    "        output = self.__net.forward()\n",
    "        output_prob = output['prob'][0]\n",
    "        return output_prob\n",
    "    \n",
    "    \n",
    "    # OTHER\n",
    "    \n",
    "    \n",
    "    def get_last_layer_size(self):\n",
    "        output = self.__net.forward()\n",
    "        output_prob = output['prob'][0]\n",
    "        return len(output_prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg10_proto = 'proto/vgg10_test.prototxt'\n",
    "vgg10_weigts = 'vgg10_100k_lr1e-1_iter_100000.caffemodel'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"FUNCTION FOR GETTING ARRAY OF LINKS TO IMAGES\"\n",
    "import os\n",
    "\n",
    "def get_links_to_img(path):\n",
    "    \"\"\"Return array of links to images\"\"\"\n",
    "    link_arr = []\n",
    "    for root, dirs, files in os.walk(top = path):\n",
    "       for filename in files:\n",
    "        link_arr.append(path+'/'+filename)\n",
    "    return link_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1043600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "TestImagesSet = '/mnt/data/dendriticDataSet'\n",
    "image_path_arr = get_links_to_img(TestImagesSet)\n",
    "print(len(image_path_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/data/dendriticDataSet/Crystal_F32_IMC=0.11_IDC=0.3_IEC=0.06_Step=76111_ID=4_crop134_.png'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg10 = CaffeNet(vgg10_proto, vgg10_weigts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images: ##################################################\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (224,224,3) into shape (32,1,224,224)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d47f8ac88fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvgg10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-da81784bd6ec>\u001b[0m in \u001b[0;36mpredict_multi\u001b[0;34m(self, image_path_arr, image_arr)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransformed_image_r\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransformed_image_arr_r\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__predict_transfromed_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_image_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mCaffeNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__show_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Predicting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-da81784bd6ec>\u001b[0m in \u001b[0;36m__predict_transfromed_image\u001b[0;34m(self, transformed_image)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mPredict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0moutput_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prob'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (224,224,3) into shape (32,1,224,224)"
     ]
    }
   ],
   "source": [
    "vgg10.predict_multi(image_path_arr[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "# For loading dendrite images\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    df = pd.read_csv('/home/student401/study/data_sets/mnist/train.csv')\n",
    "    data = df.values\n",
    "\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def one_hot_encoding(Y):\n",
    "    N = len(Y)\n",
    "    D = len(set(Y))\n",
    "    new_Y = np.zeros((N, D))\n",
    "\n",
    "    for i in range(N):\n",
    "        new_Y[i, Y[i]] = 1\n",
    "\n",
    "    return new_Y\n",
    "\n",
    "# For denrites\n",
    "def one_hot_encoding_d(Y):\n",
    "    N = len(Y)\n",
    "    D = len(set(Y))\n",
    "    new_Y = np.zeros((N, D))\n",
    "\n",
    "    for i in range(N):\n",
    "        new_Y[i, Y[i] - 1] = 1\n",
    "\n",
    "    return new_Y\n",
    "\n",
    "\n",
    "def get_preprocessed_data():\n",
    "    X, Y = get_data()\n",
    "    X = X / 255\n",
    "    Y = one_hot_encoding(Y)\n",
    "    X, Y = shuffle(X, Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def error_rate(Y, T):\n",
    "    return np.mean(Y != T)\n",
    "\n",
    "\n",
    "def create_graph(value, name, y_label, x_label):\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(16, 9)\n",
    "\n",
    "    axes = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    axes.plot(value)\n",
    "    axes.set_ylabel(y_label)\n",
    "    axes.set_xlabel(x_label)\n",
    "\n",
    "    fig.savefig(name)\n",
    "\n",
    "\n",
    "def rearrange(X):\n",
    "    # input is (N, 784)\n",
    "    # output is (N, 28, 28)\n",
    "    new_X = np.zeros((X.shape[0], 1, 28, 28))\n",
    "    for pixels_row, new_X_mat in zip(X, new_X):\n",
    "        for j in range(28):\n",
    "            new_X_mat[0, j] += pixels_row[j*28:(j+1)*28]\n",
    "\n",
    "    return new_X.astype(np.float32)\n",
    "\n",
    "\n",
    "def rearrange_tf(X):\n",
    "    # input is (N, 784)\n",
    "    # output is (N, 28, 28, 1)\n",
    "    new_X = np.zeros((X.shape[0], 28, 28, 1))\n",
    "    for j in range(28):\n",
    "        new_X[:,j,:,0] += X[:,j*28:(j+1)*28]\n",
    "\n",
    "    return new_X\n",
    "\n",
    "def rearrange_dendrites_tf(X):\n",
    "    # input is (N, 224, 224)\n",
    "    # output is (N, 224, 224, 1)\n",
    "    new_X = np.zeros((X.shape[0], 224, 224, 1))\n",
    "    new_X[:, :, :, 0] = X[:, :, :]\n",
    "\n",
    "    return new_X\n",
    "\n",
    "def rearrange_dendrites_caffe(X):\n",
    "    # input is (N, 224, 224)\n",
    "    # output is (N, 1, 224, 224)\n",
    "    new_X = np.zeros((X.shape[0], 1, 224, 224))\n",
    "    new_X[:, 0, :, :] = X[:, :, :]\n",
    "\n",
    "    return new_X\n",
    "\n",
    "\n",
    "def get_preprocessed_image_data():\n",
    "    X, Y = get_preprocessed_data()\n",
    "    X = rearrange(X)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def get_preprocessed_image_data_tf():\n",
    "    X, Y = get_preprocessed_data()\n",
    "    X = rearrange_tf(X)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def load_train_dendrit_data():\n",
    "    prepath = '/home/rustam2/nvcaffeExample/NvCaffeDendriticStructureClassifier/ResNetUsage/'\n",
    "    path_to_train_txt = '/home/student401/study/data_sets/dendrits/train.txt'\n",
    "\n",
    "    file = open(path_to_train_txt)\n",
    "    text = file.read()\n",
    "    text = text.split('\\n')[:-1]\n",
    "\n",
    "    image_paths = []\n",
    "    classes = []\n",
    "    for pair in text:\n",
    "        path, cl = pair.split(' ')\n",
    "        image_paths.append(path)\n",
    "        classes.append(cl)\n",
    "\n",
    "    for i in range(len(image_paths)):\n",
    "        image_paths[i] = prepath + image_paths[i]\n",
    "\n",
    "    images = [misc.imread(path) for path in image_paths]\n",
    "    images = np.asarray(images)\n",
    "\n",
    "    # Convert classes to numbers since they are strings\n",
    "    Y = []\n",
    "    for i in range(len(classes)):\n",
    "        Y.append(int(classes[i]))\n",
    "\n",
    "    return images, Y\n",
    "\n",
    "def load_test_dendrit_data():\n",
    "    #prepath = '/home/rustam2/nvcaffeExample/NvCaffeDendriticStructureClassifier/ResNetUsage/'\n",
    "    path_to_train_txt = '/home/rustam2/nvcaffeExample/NvCaffeDendriticStructureClassifier/ResNetUsage/resources/train.txt'\n",
    "\n",
    "    file = open(path_to_train_txt)\n",
    "    text = file.read()\n",
    "    text = text.split('\\n')[:-1]\n",
    "\n",
    "    image_paths = []\n",
    "    classes = []\n",
    "    for pair in text[:5000]:\n",
    "        path, cl = pair.split(' ')\n",
    "        image_paths.append(path)\n",
    "        classes.append(cl)\n",
    "\n",
    "    for i in range(len(image_paths)):\n",
    "        image_paths[i] = image_paths[i]\n",
    "\n",
    "    images = [misc.imread(path) for path in image_paths]\n",
    "    images = np.asarray(images)\n",
    "\n",
    "    # Convert classes to numbers since they are strings\n",
    "    Y = []\n",
    "    for i in range(len(classes)):\n",
    "        Y.append(int(classes[i]))\n",
    "\n",
    "    return images, Y\n",
    "\n",
    "def get_dendrite_preprocessed_train_data_tf():\n",
    "    # X shape is 224x224\n",
    "    X, Y = load_train_dendrit_data()\n",
    "\n",
    "    # Preprocess X\n",
    "    X = X / 255\n",
    "    X = rearrange_dendrites_tf(X)\n",
    "\n",
    "    # Convert to np array since Y is vanilla python list\n",
    "    Y = np.array(Y)\n",
    "    Y = one_hot_encoding_d(Y)\n",
    "    return X, Y\n",
    "\n",
    "def get_dendrite_preprocessed_test_data_tf():\n",
    "    # X shape is 224x224\n",
    "    X, Y = load_test_dendrit_data()\n",
    "\n",
    "    # Preprocess X\n",
    "    X = X / 255\n",
    "    X = rearrange_dendrites_tf(X)\n",
    "\n",
    "    # Convert to np array since Y is vanilla python list\n",
    "    Y = np.array(Y)\n",
    "    Y = one_hot_encoding_d(Y)\n",
    "    return X, Y\n",
    "\n",
    "def get_dendrite_preprocessed_test_data_caffe():\n",
    "    # X shape is 224x224\n",
    "    X, Y = load_test_dendrit_data()\n",
    "\n",
    "    # Preprocess X\n",
    "    X = X / 255\n",
    "    X = rearrange_dendrites_caffe(X)\n",
    "\n",
    "    # Convert to np array since Y is vanilla python list\n",
    "    Y = np.array(Y)\n",
    "    Y = one_hot_encoding_d(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rustam2/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:161: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_dendrite_preprocessed_test_data_caffe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1, 224, 224)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(caffe_net, images):\n",
    "    batch_sz = 32\n",
    "    n_batches = images.shape[0] // batch_sz\n",
    "    net = caffe_net.get_inner_net()\n",
    "    predictions = []\n",
    "    for i in range(n_batches):\n",
    "        Xbatch = images[i*batch_sz:(i+1)*batch_sz]\n",
    "        net.blobs['data'].data[...] = Xbatch\n",
    "        output = net.forward()\n",
    "        #output = net.blobs['score'].data[...]\n",
    "        for prob in output['score'][::-1]:\n",
    "            predictions.append(prob.argmax())\n",
    "        \n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_rate(Y, T):\n",
    "    return np.mean(Y == T)\n",
    "\n",
    "predictions2 = predict(vgg10, X[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [1]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1],[2]])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  6,  0,  6,  4,  0,  5,  7,  8,  8, 10,  2, 10,  0,  9,  2,  2,\n",
       "        3,  8,  8,  9,  7,  2,  6,  0,  0, 10,  5,  4,  7,  4,  6])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Y[:32] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09979838709677419"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_rate(np.array(predictions2), np.argmax(Y[:992], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', (32, 1, 224, 224)),\n",
       " ('label', (32,)),\n",
       " ('label_data_1_split_0', (32,)),\n",
       " ('label_data_1_split_1', (32,)),\n",
       " ('label_data_1_split_2', (32,)),\n",
       " ('conv1_1', (32, 64, 224, 224)),\n",
       " ('conv1_2', (32, 64, 224, 224)),\n",
       " ('pool1', (32, 64, 112, 112)),\n",
       " ('conv2_1', (32, 128, 112, 112)),\n",
       " ('conv2_2', (32, 128, 112, 112)),\n",
       " ('pool2', (32, 128, 56, 56)),\n",
       " ('conv3_1', (32, 256, 56, 56)),\n",
       " ('conv3_2', (32, 256, 56, 56)),\n",
       " ('conv3_3', (32, 256, 56, 56)),\n",
       " ('pool3', (32, 256, 28, 28)),\n",
       " ('conv4_1', (32, 512, 28, 28)),\n",
       " ('conv4_2', (32, 512, 28, 28)),\n",
       " ('conv4_3', (32, 512, 28, 28)),\n",
       " ('pool4', (32, 512, 14, 14)),\n",
       " ('conv5_1', (32, 512, 14, 14)),\n",
       " ('conv5_2', (32, 512, 14, 14)),\n",
       " ('conv5_3', (32, 512, 14, 14)),\n",
       " ('pool5', (32, 512, 7, 7)),\n",
       " ('fc1', (32, 4096)),\n",
       " ('fc2', (32, 2048)),\n",
       " ('score', (32, 11)),\n",
       " ('score_score_0_split_0', (32, 11)),\n",
       " ('score_score_0_split_1', (32, 11)),\n",
       " ('score_score_0_split_2', (32, 11)),\n",
       " ('loss', ()),\n",
       " ('accuracy_top1', ()),\n",
       " ('accuracy_top5', ())]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v.data.shape) for k, v in vgg10.get_inner_net().blobs.items()]\n",
    "#predictions = predict(vgg10, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE DATAFRAMES WITH ANSWERS OF EACH NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student401/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \n",
      "/home/student401/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "columns = ['GOOGLENET','ALEXNET','VGG16','RESNET']\n",
    "rows = image_path_arr\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(len(rows),len(columns)), rows, columns)\n",
    "df.to_csv('confidence.csv')\n",
    "df.to_csv('labels.csv')\n",
    "\n",
    "df_confidence = pd.DataFrame.from_csv('confidence.csv')\n",
    "df_labels = pd.DataFrame.from_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_each_neuron_data(net_tuple, columns):\n",
    "    net = CaffeNet(net_tuple[0], net_tuple[1])\n",
    "    last_layer_size = net.get_last_layer_size()\n",
    "    \n",
    "    rows = np.array(range(last_layer_size))\n",
    "    \n",
    "    df = pd.DataFrame(np.random.randn(len(rows),len(columns)), rows, columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlenet = df_each_neuron_data(googlenet, image_path_arr)\n",
    "df_alexnet = df_each_neuron_data(alexnet, image_path_arr)\n",
    "df_vgg16 = df_each_neuron_data(vgg16, image_path_arr)\n",
    "df_resnet = df_each_neuron_data(resnet152, image_path_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE RESULTS FROM EACH NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction_to_df(df, column_name, answer_arr, mark0_conf1 = 0):\n",
    "    \"\"\"\n",
    "    This function save prediction results to dataframe df.\n",
    "    mark0_conf1: 0 - save label, 1 - save confidence\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(df[column_name])):\n",
    "        df[column_name][i] = answer_arr[i][mark0_conf1]\n",
    "        \n",
    "def save_prediction_full_data_to_df(df, answer_full_data_arr, image_path_arr):\n",
    "    \"\"\"\n",
    "    This function save full data from a certain net to dataframe\n",
    "    \"\"\"\n",
    "    for i in range(len(df_googlenet.iloc[0])):\n",
    "        for j in range(len(df[image_path_arr[0]])):\n",
    "            df[image_path_arr[i]][j] = answer_full_data_arr[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images: ##################################################\n"
     ]
    }
   ],
   "source": [
    "# image_path_arr declared above\n",
    "image_path_arr\n",
    "columns = ['GOOGLENET','ALEXNET','VGG16','RESNET']\n",
    "net_arr = [googlenet, alexnet, vgg16, resnet152]\n",
    "df_full_data_arr = [df_googlenet, df_alexnet, df_vgg16, df_resnet]\n",
    "loaded_image_arr = CaffeNet.load_multiple_images(image_path_arr=image_path_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: ##################################################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(4):\n",
    "    # Load and compile net\n",
    "    net = CaffeNet(net_arr[i][0], net_arr[i][1])\n",
    "    # Get answers\n",
    "    answer_arr_full_data = net.predict_multi_full_data(image_path_arr, loaded_image_arr)\n",
    "    answer_arr = CaffeNet.convert_full_data_to_simple(answer_arr_full_data)\n",
    "    # Save simple results\n",
    "    save_prediction_to_df(df_labels, columns[i], answer_arr, 0)\n",
    "    save_prediction_to_df(df_confidence, columns[i], answer_arr, 1)\n",
    "    # Save full data results\n",
    "    save_prediction_full_data_to_df(df_full_data_arr[i], answer_arr_full_data, image_path_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confidence.to_csv('confidence.csv')\n",
    "df_labels.to_csv('labels.csv')\n",
    "\n",
    "df_googlenet.to_csv('googlenet.csv')\n",
    "df_alexnet.to_csv('alexnet.csv')\n",
    "df_vgg16.to_csv('vgg16.csv')\n",
    "df_resnet.to_csv('resnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GOOGLENET</th>\n",
       "      <th>ALEXNET</th>\n",
       "      <th>VGG16</th>\n",
       "      <th>RESNET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00020611.jpg</th>\n",
       "      <td>145.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00027736.jpg</th>\n",
       "      <td>359.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>359.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00010687.jpg</th>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00018876.jpg</th>\n",
       "      <td>195.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00004086.jpg</th>\n",
       "      <td>215.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>215.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    GOOGLENET  ALEXNET  VGG16  \\\n",
       "//home//student401//neuralnets//DataSet//val_25...      145.0    197.0  145.0   \n",
       "//home//student401//neuralnets//DataSet//val_25...      359.0    359.0  359.0   \n",
       "//home//student401//neuralnets//DataSet//val_25...       42.0     42.0   42.0   \n",
       "//home//student401//neuralnets//DataSet//val_25...      195.0    195.0  195.0   \n",
       "//home//student401//neuralnets//DataSet//val_25...      215.0    121.0  215.0   \n",
       "\n",
       "                                                    RESNET  \n",
       "//home//student401//neuralnets//DataSet//val_25...   145.0  \n",
       "//home//student401//neuralnets//DataSet//val_25...   359.0  \n",
       "//home//student401//neuralnets//DataSet//val_25...    42.0  \n",
       "//home//student401//neuralnets//DataSet//val_25...   195.0  \n",
       "//home//student401//neuralnets//DataSet//val_25...   215.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GOOGLENET</th>\n",
       "      <th>ALEXNET</th>\n",
       "      <th>VGG16</th>\n",
       "      <th>RESNET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00020611.jpg</th>\n",
       "      <td>0.856080</td>\n",
       "      <td>0.350724</td>\n",
       "      <td>0.806077</td>\n",
       "      <td>0.529477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00027736.jpg</th>\n",
       "      <td>0.444585</td>\n",
       "      <td>0.491834</td>\n",
       "      <td>0.529767</td>\n",
       "      <td>0.843238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00010687.jpg</th>\n",
       "      <td>0.667495</td>\n",
       "      <td>0.547526</td>\n",
       "      <td>0.838639</td>\n",
       "      <td>0.926617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00018876.jpg</th>\n",
       "      <td>0.972118</td>\n",
       "      <td>0.941433</td>\n",
       "      <td>0.976265</td>\n",
       "      <td>0.997452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00004086.jpg</th>\n",
       "      <td>0.398947</td>\n",
       "      <td>0.294820</td>\n",
       "      <td>0.764184</td>\n",
       "      <td>0.393228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    GOOGLENET   ALEXNET  \\\n",
       "//home//student401//neuralnets//DataSet//val_25...   0.856080  0.350724   \n",
       "//home//student401//neuralnets//DataSet//val_25...   0.444585  0.491834   \n",
       "//home//student401//neuralnets//DataSet//val_25...   0.667495  0.547526   \n",
       "//home//student401//neuralnets//DataSet//val_25...   0.972118  0.941433   \n",
       "//home//student401//neuralnets//DataSet//val_25...   0.398947  0.294820   \n",
       "\n",
       "                                                       VGG16    RESNET  \n",
       "//home//student401//neuralnets//DataSet//val_25...  0.806077  0.529477  \n",
       "//home//student401//neuralnets//DataSet//val_25...  0.529767  0.843238  \n",
       "//home//student401//neuralnets//DataSet//val_25...  0.838639  0.926617  \n",
       "//home//student401//neuralnets//DataSet//val_25...  0.976265  0.997452  \n",
       "//home//student401//neuralnets//DataSet//val_25...  0.764184  0.393228  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confidence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00020611.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00027736.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00010687.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00018876.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00004086.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00020247.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00032581.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00003318.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00034661.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00018961.jpg</th>\n",
       "      <th>...</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00014336.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00008776.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00006841.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00015991.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00025377.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00021812.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00005874.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00006424.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00027391.jpg</th>\n",
       "      <th>//home//student401//neuralnets//DataSet//val_256//Places365_val_00001122.jpg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.159526e-07</td>\n",
       "      <td>1.543440e-04</td>\n",
       "      <td>7.339061e-06</td>\n",
       "      <td>8.759868e-09</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.162869e-08</td>\n",
       "      <td>3.721017e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.997119e-07</td>\n",
       "      <td>7.926048e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>7.761012e-07</td>\n",
       "      <td>1.223886e-06</td>\n",
       "      <td>1.539239e-05</td>\n",
       "      <td>4.986495e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>4.574176e-07</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>9.738069e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.975498e-09</td>\n",
       "      <td>1.205092e-06</td>\n",
       "      <td>3.664356e-08</td>\n",
       "      <td>4.173852e-07</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>2.951170e-08</td>\n",
       "      <td>4.130065e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>2.866622e-08</td>\n",
       "      <td>2.116008e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.981672e-08</td>\n",
       "      <td>1.735944e-05</td>\n",
       "      <td>5.106659e-08</td>\n",
       "      <td>1.045868e-06</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>1.602975e-07</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>1.248269e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.099129e-07</td>\n",
       "      <td>1.720226e-06</td>\n",
       "      <td>7.896384e-07</td>\n",
       "      <td>8.642601e-07</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>4.284949e-09</td>\n",
       "      <td>2.011322e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>8.860980e-06</td>\n",
       "      <td>1.211861e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>3.958298e-08</td>\n",
       "      <td>6.013190e-05</td>\n",
       "      <td>8.487540e-06</td>\n",
       "      <td>8.851688e-08</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>1.128363e-07</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>1.677633e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.276428e-07</td>\n",
       "      <td>2.392427e-07</td>\n",
       "      <td>2.638800e-07</td>\n",
       "      <td>2.374833e-05</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>8.817320e-07</td>\n",
       "      <td>1.485833e-05</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5.312104e-08</td>\n",
       "      <td>1.356587e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.732927e-07</td>\n",
       "      <td>6.878187e-06</td>\n",
       "      <td>2.751497e-07</td>\n",
       "      <td>2.705490e-05</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>6.691239e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.036851e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.075771e-06</td>\n",
       "      <td>1.531285e-06</td>\n",
       "      <td>2.727855e-07</td>\n",
       "      <td>9.034971e-09</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2.364391e-07</td>\n",
       "      <td>6.416658e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.343919e-05</td>\n",
       "      <td>4.453065e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>5.080334e-07</td>\n",
       "      <td>8.908675e-07</td>\n",
       "      <td>8.741235e-08</td>\n",
       "      <td>1.012164e-04</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>1.176754e-04</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>2.573156e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00020611.jpg  \\\n",
       "0                                       2.159526e-07                              \n",
       "1                                       6.975498e-09                              \n",
       "2                                       1.099129e-07                              \n",
       "3                                       4.276428e-07                              \n",
       "4                                       3.075771e-06                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00027736.jpg  \\\n",
       "0                                       1.543440e-04                              \n",
       "1                                       1.205092e-06                              \n",
       "2                                       1.720226e-06                              \n",
       "3                                       2.392427e-07                              \n",
       "4                                       1.531285e-06                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00010687.jpg  \\\n",
       "0                                       7.339061e-06                              \n",
       "1                                       3.664356e-08                              \n",
       "2                                       7.896384e-07                              \n",
       "3                                       2.638800e-07                              \n",
       "4                                       2.727855e-07                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00018876.jpg  \\\n",
       "0                                       8.759868e-09                              \n",
       "1                                       4.173852e-07                              \n",
       "2                                       8.642601e-07                              \n",
       "3                                       2.374833e-05                              \n",
       "4                                       9.034971e-09                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00004086.jpg  \\\n",
       "0                                           0.000003                              \n",
       "1                                           0.000015                              \n",
       "2                                           0.000019                              \n",
       "3                                           0.003146                              \n",
       "4                                           0.000004                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00020247.jpg  \\\n",
       "0                                       2.162869e-08                              \n",
       "1                                       2.951170e-08                              \n",
       "2                                       4.284949e-09                              \n",
       "3                                       8.817320e-07                              \n",
       "4                                       2.364391e-07                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00032581.jpg  \\\n",
       "0                                       3.721017e-06                              \n",
       "1                                       4.130065e-07                              \n",
       "2                                       2.011322e-06                              \n",
       "3                                       1.485833e-05                              \n",
       "4                                       6.416658e-07                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00003318.jpg  \\\n",
       "0                                           0.000007                              \n",
       "1                                           0.000005                              \n",
       "2                                           0.000003                              \n",
       "3                                           0.000003                              \n",
       "4                                           0.000001                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00034661.jpg  \\\n",
       "0                                       1.997119e-07                              \n",
       "1                                       2.866622e-08                              \n",
       "2                                       8.860980e-06                              \n",
       "3                                       5.312104e-08                              \n",
       "4                                       1.343919e-05                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00018961.jpg  \\\n",
       "0                                       7.926048e-07                              \n",
       "1                                       2.116008e-05                              \n",
       "2                                       1.211861e-04                              \n",
       "3                                       1.356587e-03                              \n",
       "4                                       4.453065e-07                              \n",
       "\n",
       "                                       ...                                       \\\n",
       "0                                      ...                                        \n",
       "1                                      ...                                        \n",
       "2                                      ...                                        \n",
       "3                                      ...                                        \n",
       "4                                      ...                                        \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00014336.jpg  \\\n",
       "0                                           0.000049                              \n",
       "1                                           0.000025                              \n",
       "2                                           0.000036                              \n",
       "3                                           0.000001                              \n",
       "4                                           0.000009                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00008776.jpg  \\\n",
       "0                                           0.000002                              \n",
       "1                                           0.000005                              \n",
       "2                                           0.000002                              \n",
       "3                                           0.000002                              \n",
       "4                                           0.000002                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00006841.jpg  \\\n",
       "0                                       7.761012e-07                              \n",
       "1                                       1.981672e-08                              \n",
       "2                                       3.958298e-08                              \n",
       "3                                       1.732927e-07                              \n",
       "4                                       5.080334e-07                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00015991.jpg  \\\n",
       "0                                       1.223886e-06                              \n",
       "1                                       1.735944e-05                              \n",
       "2                                       6.013190e-05                              \n",
       "3                                       6.878187e-06                              \n",
       "4                                       8.908675e-07                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00025377.jpg  \\\n",
       "0                                       1.539239e-05                              \n",
       "1                                       5.106659e-08                              \n",
       "2                                       8.487540e-06                              \n",
       "3                                       2.751497e-07                              \n",
       "4                                       8.741235e-08                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00021812.jpg  \\\n",
       "0                                       4.986495e-07                              \n",
       "1                                       1.045868e-06                              \n",
       "2                                       8.851688e-08                              \n",
       "3                                       2.705490e-05                              \n",
       "4                                       1.012164e-04                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00005874.jpg  \\\n",
       "0                                           0.000006                              \n",
       "1                                           0.000013                              \n",
       "2                                           0.000078                              \n",
       "3                                           0.000116                              \n",
       "4                                           0.000165                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00006424.jpg  \\\n",
       "0                                       4.574176e-07                              \n",
       "1                                       1.602975e-07                              \n",
       "2                                       1.128363e-07                              \n",
       "3                                       6.691239e-06                              \n",
       "4                                       1.176754e-04                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00027391.jpg  \\\n",
       "0                                           0.000487                              \n",
       "1                                           0.000096                              \n",
       "2                                           0.000502                              \n",
       "3                                           0.000002                              \n",
       "4                                           0.000115                              \n",
       "\n",
       "   //home//student401//neuralnets//DataSet//val_256//Places365_val_00001122.jpg  \n",
       "0                                       9.738069e-08                             \n",
       "1                                       1.248269e-06                             \n",
       "2                                       1.677633e-02                             \n",
       "3                                       1.036851e-06                             \n",
       "4                                       2.573156e-06                             \n",
       "\n",
       "[5 rows x 36500 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_googlenet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST CELLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36500\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(loaded_image_arr[1])\n",
    "print(len(loaded_image_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = CaffeNet(alexnet_def, alexnet_w)\n",
    "# HERE IS A PROBLEM\n",
    "answer_arr_full_data = alexnet.predict_multi_full_data(image_path_arr, loaded_image_arr)\n",
    "#print((answer_arr_full_data_tt))\n",
    "answer_arr = CaffeNet.convert_full_data_to_simple(answer_arr_full_data_tt)\n",
    "print((answer_arr))\n",
    "\n",
    "#print(alexnet.convert_full_data_to_simple(alexnet.predict_multi_full_data(image_path_arr, loaded_image_arr[:-98])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answer_arr_tt[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in range(len(answer_arr)):\n",
    "    start_time = time.time()\n",
    "    for j in range(len(df_alexnet[image_path_arr[0]])):\n",
    "        df_alexnet[image_path_arr[i]][j] = answer_arr_full_data[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alexnet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_arr = CaffeNet.convert_full_data_to_simple(answer_arr_full_data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
